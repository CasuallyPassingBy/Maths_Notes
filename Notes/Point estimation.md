---
tags:
  - Statistics
---
Links: [[Statistics and Sample Distribution]]

Let $X_1, \dots, X_n$ be a random sample form a population with density $f(x;\theta)$, with $\theta$ being unknown. We would like to try calculate, through the sample values, a statistic $\hat \theta = T(X_1, \dots, X_n)$, that assigns a value to the unknown value of the population, in a way that might be the closes in some sense. $\hat \theta$ is called an *estimator*

The objective in *point estimation* is to give a value of $\theta$, denoted $\hat \theta$, that is function from the random sample that let's describe the random phenomena. 

**Def:** Let $X_1, \dots, X_n$ be a random sample from a population with density values $f(x; \theta)$. An estimator is a statistic $T(\underline X)$ which values $t(\underline x)$ are useful to approximate or estimate the values of $\theta$. 

There are different estimations methods:
- [[Method of Moments]]
- [[Maximum Likelihood estimators]]
- [[Bayesian Approach to Point Estimators]]